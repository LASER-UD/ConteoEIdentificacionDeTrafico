{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "TransferLearning_TipoVehiculo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVSTDkb_EzGG"
      },
      "source": [
        "!pip install pyunpack\r\n",
        "!pip install patool"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKF7khHPE0aN"
      },
      "source": [
        "#Importación de librerías necesarias\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "import tensorflow as tf\r\n",
        "from pyunpack import Archive\r\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\r\n",
        "from keras.callbacks import ModelCheckpoint\r\n",
        "from tensorflow.keras.models import load_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrJLW48nLB9i"
      },
      "source": [
        "#Importamos las librerías de costumbre\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "#Instalamos la librería que nos va a permitir bajar de drive\r\n",
        "!pip install gdown\r\n",
        "import gdown\r\n",
        "#Colocamos la URL del archivo\r\n",
        "url = 'https://drive.google.com/uc?id=1LHK1UqsOZMlF9lFuMeRApw6P5mkkexnM'\r\n",
        "#Colocamos el nombre al archivo donde la información será guardada\r\n",
        "Nombre = 'TipoVehiculo.rar'\r\n",
        "#Lo descargamos\r\n",
        "gdown.download(url,Nombre,quiet=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYXaFyIYE5n9"
      },
      "source": [
        "#Descompresión de las imágenes\r\n",
        "os.mkdir('dataset')\r\n",
        "Archive('TipoVehiculo.rar').extractall('dataset/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oi8qp0BaFIAe"
      },
      "source": [
        "#Se cargan los conjustos de imágenes y se reescalan\r\n",
        "\r\n",
        "train_dir = os.path.join('dataset/TipoVehiculo')\r\n",
        "\r\n",
        "BATCH_SIZE = 15\r\n",
        "IMG_SIZE = (224, 224)\r\n",
        "\r\n",
        "train_dataset = image_dataset_from_directory(train_dir,\r\n",
        "                                             shuffle=True,\r\n",
        "                                             batch_size=BATCH_SIZE,\r\n",
        "                                             image_size=IMG_SIZE,\r\n",
        "                                             label_mode='categorical')\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X64fa1QOQxSF"
      },
      "source": [
        "#Se crea el dataset de validación\r\n",
        "\r\n",
        "train_batches = tf.data.experimental.cardinality(train_dataset)\r\n",
        "validation_dataset = train_dataset.take(train_batches // 4)\r\n",
        "train_dataset = train_dataset.skip(train_batches // 4)\r\n",
        "\r\n",
        "print('Number of validation batches: %d' % tf.data.experimental.cardinality(validation_dataset))\r\n",
        "print('Number of test batches: %d' % tf.data.experimental.cardinality(train_dataset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlAMR8QXFcxH"
      },
      "source": [
        "#Configuración de los conjuntos de datos para mejor el rendimiento en el entrenamiento usando la API tf.data\r\n",
        "\r\n",
        "AUTOTUNE = tf.data.AUTOTUNE\r\n",
        "\r\n",
        "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\r\n",
        "validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPoDUJz-F4K9"
      },
      "source": [
        "#Usar aumento de datos\r\n",
        "\r\n",
        "data_augmentation = tf.keras.Sequential([\r\n",
        "  tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\r\n",
        "  tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\r\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IItOB-rFdqO"
      },
      "source": [
        "#Cambiar la escala de los valores de píxeles de las imágenes \r\n",
        "\r\n",
        "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\r\n",
        "rescale = tf.keras.layers.experimental.preprocessing.Rescaling(1./127.5, offset= -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEuoOQ9UFuTF"
      },
      "source": [
        "# Creación sel modelo base a partir del modelo previamente entrenado MobileNet V2\r\n",
        "IMG_SHAPE = IMG_SIZE + (3,)\r\n",
        "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\r\n",
        "                                               include_top=False,\r\n",
        "                                               weights='imagenet')\r\n",
        "\r\n",
        "#Congelar la base convolucional\r\n",
        "\r\n",
        "base_model.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zjtBJINFyPs"
      },
      "source": [
        "#Agregar un encabezado de clasificación\r\n",
        "\r\n",
        "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\r\n",
        "prediction_layer = tf.keras.layers.Dense(8,activation='softmax',kernel_initializer='random_normal',bias_initializer='zeros')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HRoP8sj4cS7"
      },
      "source": [
        "import json\r\n",
        "\r\n",
        "def write_json(dataz, filename): \r\n",
        "\twith open(filename,'w') as f: \r\n",
        "\t    json.dump(dataz, f, indent=4) \r\n",
        "\r\n",
        "data = []\r\n",
        "\r\n",
        "with open('history.json', 'w') as file:\r\n",
        "    json.dump(data, file, indent=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIwM2E9jF_--"
      },
      "source": [
        "#Compilar el modelo y entrenarlo\r\n",
        "\r\n",
        "os.mkdir('modelos')\r\n",
        "base_learning_rate = 0.0001\r\n",
        "history = []\r\n",
        "contador = 0\r\n",
        "\r\n",
        "for i in range(1,101):\r\n",
        "    inputs = tf.keras.Input(shape=(224, 224, 3))\r\n",
        "    x = data_augmentation(inputs)\r\n",
        "    x = preprocess_input(x)\r\n",
        "    x = base_model(x, training=False)\r\n",
        "    x = global_average_layer(x)\r\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\r\n",
        "    outputs = prediction_layer(x)\r\n",
        "    model = tf.keras.Model(inputs, outputs)\r\n",
        "    mcp_save = ModelCheckpoint('/content/modelos/transfer_learning_TV_'+str(i)+'.hdf5', save_best_only=True, monitor='val_loss', mode='min',save_weights_only=False)\r\n",
        "    model.compile(optimizer=tf.keras.optimizers.SGD(lr=base_learning_rate),\r\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\r\n",
        "              metrics=['accuracy'])\r\n",
        "    history.append(model.fit(train_dataset,epochs=100,callbacks=[mcp_save],validation_data=validation_dataset)) \r\n",
        "\r\n",
        "    with open('history.json') as json_file: \r\n",
        "\t    datay = json.load(json_file) \r\n",
        "     \r\n",
        "\t    temp = datay\r\n",
        "\r\n",
        "\t    y = { 'accuracy': history[contador].history['accuracy'],\r\n",
        "            'loss': history[contador].history['val_loss']\r\n",
        "      }        \t\t     \r\n",
        "\r\n",
        "\t    temp.append(y) \r\n",
        "    write_json(datay, 'history.json')  \r\n",
        " \r\n",
        "    contador = contador + 1\r\n",
        "\r\n",
        "    del model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFaLpxP-GAis"
      },
      "source": [
        "#Visualización de los resultados en gráficas\r\n",
        "\r\n",
        "for i in range(len(history)):\r\n",
        "  plt.plot(history[i].history['accuracy'])\r\n",
        "plt.title('Exactitud del modelo')\r\n",
        "plt.ylabel('Exactitud')\r\n",
        "plt.xlabel('Época')\r\n",
        "plt.show()\r\n",
        "# \"Loss\"\r\n",
        "for i in range(len(history)):\r\n",
        "  plt.plot(history[i].history['val_loss'])\r\n",
        "plt.title('Métrica de pérdida')\r\n",
        "plt.ylabel('Pérdida')\r\n",
        "plt.xlabel('Época')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1auGTORGCJ0"
      },
      "source": [
        "#calculo del porcentajes de validación \r\n",
        "\r\n",
        "best_val_history = []\r\n",
        "for i in range(1,101):\r\n",
        "    model = load_model('modelos_TV/transfer_learning_TV_'+str(i)+'.hdf5')\r\n",
        "    best_val_history.append(model.evaluate(validation_dataset))\r\n",
        "    del model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_6lBjFkGDyO"
      },
      "source": [
        "#Calculo de promedios de validación y pérdida \r\n",
        "\r\n",
        "prom_loss = 0.0;\r\n",
        "prom_acc = 0.0;\r\n",
        "for i in range(len(best_val_history)):\r\n",
        "    prom_loss = prom_loss + best_val_history[i][0]\r\n",
        "    prom_acc = prom_acc + best_val_history[i][1]\r\n",
        "\r\n",
        "prom_loss = prom_loss / len(best_val_history)\r\n",
        "prom_acc = prom_acc / len(best_val_history)\r\n",
        "\r\n",
        "print(prom_loss)\r\n",
        "print(prom_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKxpuBxfGGlM"
      },
      "source": [
        "import shutil\r\n",
        "shutil.make_archive(\"modelos_tipo\", 'zip', \"modelos\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}